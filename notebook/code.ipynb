{"cells":[{"cell_type":"markdown","metadata":{"id":"rRw9K42QolLz"},"source":["# Task 4"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"TAwNn9HBolL2","executionInfo":{"status":"ok","timestamp":1709793884198,"user_tz":480,"elapsed":7157,"user":{"displayName":"Kayden Lea","userId":"11902528103199788041"}},"outputId":"68648744-500c-4eec-bb6d-03b8c93fa73d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"]}],"source":["!pip install spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBCGTImAolL3","outputId":"887b643d-80ab-486d-cd52-86236e52cba3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n","Requirement already satisfied: jinja2 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n","Requirement already satisfied: setuptools in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n","Requirement already satisfied: packaging>=20.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /Users/kexuanzou/.local/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: annotated-types>=0.4.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/kexuanzou/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n","Installing collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!python -m spacy download en_core_web_sm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHxeL9M8olL5"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the updated dataframe\n","df = pd.read_csv('../Data/task_4_dataset.csv')\n","\n","# Group by \"State\" and \"County\" and count the number of sightings\n","location_counts = df.groupby(['State', 'County']).size()\n","\n","# Get the top 10 locations with the most sightings\n","top_10_locations = location_counts.nlargest(10).index\n","\n","# Define a function to determine if a sighting is in a BigFoot Hotspot\n","def is_bigfoot_hotspot(row):\n","    return (row['State'], row['County']) in top_10_locations\n","\n","# Apply the function to create the \"BigFoot Hotspot\" column\n","df['BigFoot Hotspot'] = df.apply(is_bigfoot_hotspot, axis=1)\n","\n","# Save the updated dataframe\n","df.to_csv('../Data/updated_file.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"zghm4cGsolL5","outputId":"38ffab47-5c4c-480c-e17a-a1ad0003c153"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-19-065ec2d77873>:47: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df2_unique['Year'] = pd.to_numeric(df2_unique['Year'], errors='coerce').fillna(-1).astype(int)\n","<ipython-input-19-065ec2d77873>:48: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df2_unique['Month'] = df2_unique['Month'].astype(str)\n","<ipython-input-19-065ec2d77873>:49: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df2_unique['Date'] = pd.to_numeric(df2_unique['Date'], errors='coerce').fillna(-1).astype(int)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Mapping of state abbreviations to full names\n","state_mapping = {\n","    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n","    \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\",\n","    \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\",\n","    \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n","    \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\",\n","    \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n","    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\",\n","    \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n","    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\",\n","    \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"\n","}\n","\n","def parse_submitted_date(df):\n","    df['Submitted Date'] = df['Submitted Date'].str.rstrip('.').str.strip()\n","    df['Year'] = pd.to_datetime(df['Submitted Date'], format='%A, %B %d, %Y', errors='coerce').dt.year\n","    df['Month'] = pd.to_datetime(df['Submitted Date'], format='%A, %B %d, %Y', errors='coerce').dt.month_name()\n","    df['Date'] = pd.to_datetime(df['Submitted Date'], format='%A, %B %d, %Y', errors='coerce').dt.day\n","    return df\n","\n","def preprocess_df2(df):\n","    # Map state abbreviations to full names\n","    df['State'] = df['State'].map(state_mapping)\n","    # Drop duplicate rows based on 'Year', 'Month', 'Date', 'State', and 'County' (keep only the first occurrence)\n","    return df.drop_duplicates(subset=['Year', 'Month', 'Date', 'State', 'County'], keep='first')\n","\n","def main():\n","    # Load the datasets\n","    df1 = pd.read_csv('../Data/updated_file.csv')\n","    df2 = pd.read_csv('../Data/updated_data.csv')\n","\n","    # Use ThreadPoolExecutor to parallelize the parsing and preprocessing\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        future_df1 = executor.submit(parse_submitted_date, df1)\n","        future_df2 = executor.submit(preprocess_df2, df2)\n","\n","        # Wait for the futures to complete and get the results\n","        df1 = future_df1.result()\n","        df2_unique = future_df2.result()\n","\n","    # Ensure 'Year', 'Month', 'Date' columns in df2 are in the correct format\n","    df2_unique['Year'] = pd.to_numeric(df2_unique['Year'], errors='coerce').fillna(-1).astype(int)\n","    df2_unique['Month'] = df2_unique['Month'].astype(str)\n","    df2_unique['Date'] = pd.to_numeric(df2_unique['Date'], errors='coerce').fillna(-1).astype(int)\n","\n","    # Join the dataframes on 'Year', 'Month', 'Date', 'State', and 'County' using a left join\n","    result = pd.merge(df1, df2_unique, left_on=['Year', 'Month', 'Date', 'State', 'County'], right_on=['Year', 'Month', 'Date', 'State', 'County'], how='left')\n","\n","    # Replace the placeholder (-1) with NaN where necessary\n","    result.replace(-1, np.nan, inplace=True)\n","\n","    # Save the merged dataset to a new CSV file\n","    result.to_csv('task_4_dataset.csv', index=False)\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNLu1AC6olL6","outputId":"02252ebd-a1eb-431c-9357-d93498bd3587"},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated CSV saved as updated_data.csv\n"]}],"source":["import pandas as pd\n","import datetime\n","\n","# Assuming 'data.csv' is the input CSV file\n","input_csv = 'WeatherEvents_Jan2016-Dec2022.csv'\n","output_csv = 'updated_data.csv'  # The new file with additional columns\n","\n","# Read the CSV data into a DataFrame.\n","df = pd.read_csv(input_csv, parse_dates=['StartTime(UTC)', 'EndTime(UTC)'])\n","\n","# Extract 'Month', 'Year', and 'Date' from 'StartTime(UTC)' and add as new columns.\n","df['Month'] = df['StartTime(UTC)'].dt.strftime('%B')\n","df['Year'] = df['StartTime(UTC)'].dt.year\n","df['Date'] = df['StartTime(UTC)'].dt.day\n","\n","# Save the updated dataframe to a new CSV file\n","df.to_csv(output_csv, index=False)\n","\n","print(f\"Updated CSV saved as {output_csv}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9TVkytdLolL7","executionInfo":{"status":"ok","timestamp":1709794116824,"user_tz":480,"elapsed":7313,"user":{"displayName":"Kayden Lea","userId":"11902528103199788041"}},"outputId":"3940ba0b-cd77-47f3-a4d3-7965e15654ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m174.1/227.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n","Installing collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"]}],"source":["!pip install openai\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwQJ8P-IolL7"},"outputs":[],"source":["import os\n","import pandas as pd\n","from openai import OpenAI\n","\n","# Set up the OpenAI client\n","client = OpenAI(\n","    api_key=\"key\"\n",")\n","\n","\n","# Read the CSV file\n","df = pd.read_csv('../Data/task_4_dataset.csv')\n","\n","# Function to estimate witness count using GPT-3.5\n","def estimate_witness_count(description):\n","    if not isinstance(description, str):\n","        return 0  # Return 0 if the description is not a string\n","\n","    # Formulate the question\n","    question = f\"How many people are mentioned here? Respond as just an integer: {description}\"\n","\n","    # Get the response from GPT-3.5\n","    chat_completion = client.chat.completions.create(\n","        messages=[{\"role\": \"user\", \"content\": question}],\n","        model=\"gpt-3.5-turbo\"\n","    )\n","\n","    # Extract the estimated count from the response\n","    try:\n","        count = int(chat_completion.choices[0].message.content.strip())\n","    except ValueError:\n","        count = 0  # Default to 0 if the response is not a valid integer\n","\n","    return count\n","\n","# Apply the function to the \"Other Witnesses\" column\n","df['Witness Count'] = df['Other Witnesses'].apply(estimate_witness_count)\n","\n","# Save the updated DataFrame to a new CSV file\n","df.to_csv('../Data/task_4_dataset.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pi_6cU__olL4"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the updated dataframe\n","df = pd.read_csv('../Data/task_4_dataset.csv')\n","\n","# Define a function to determine the \"Multiple Witnesses\" feature\n","def multiple_witnesses(row):\n","    if row['Witness Count'] > 2:\n","        if row['Class'] in ['Class A', 'Class B']:\n","            return True\n","        elif row['Class'] == 'Unknown':\n","            row['Class'] = 'Class B'\n","            return True\n","        elif row['Class'] == 'Class C':\n","            return True\n","    return False\n","\n","# Apply the function to create the \"Multiple Witnesses\" column\n","df['Multiple Witnesses'] = df.apply(multiple_witnesses, axis=1)\n","\n","# Save the updated dataframe\n","df.to_csv('../Data/task_4_dataset', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PltNcVTyolL6","outputId":"c85fdc67-dded-4272-9f58-6aa7df107ba3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Converted task_4_dataset.csv to task_4_dataset.tsv\n"]}],"source":["import csv\n","\n","# Input CSV file\n","input_csv_file = 'task_4_dataset.csv'\n","\n","# Output TSV file\n","output_tsv_file = 'task_4_dataset.tsv'\n","\n","# Read the CSV file and write to a TSV file\n","with open(input_csv_file, 'r') as csv_file, open(output_tsv_file, 'w', newline='') as tsv_file:\n","    csv_reader = csv.reader(csv_file)\n","    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n","\n","    for row in csv_reader:\n","        tsv_writer.writerow(row)\n","\n","print(f'Converted {input_csv_file} to {output_tsv_file}')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IchVn1qDpR3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npGF_4VGolL7"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","df = pd.read_csv('../Data/task_4_dataset.csv')\n","\n","# Convert 'Witness Count' column to integer, skipping invalid literals\n","def safe_convert_to_int(value):\n","    try:\n","        return int(value)\n","    except ValueError:\n","        return None\n","\n","df['Witness Count'] = df['Witness Count'].apply(safe_convert_to_int)\n","\n","# Define a function to set the value of 'Multiple Witnesses' and 'Class' based on the conditions\n","def set_multiple_witnesses(row):\n","    if row['Witness Count'] is None:\n","        return None, None\n","    elif row['Witness Count'] < 2:\n","        return False, row['Class']\n","    elif row['Witness Count'] > 2 and (row['Class'] == 'Class A' or row['Class'] == 'Class B'):\n","        return True, row['Class']\n","    elif row['Class'] == 'Unknown':\n","        return True, 'Class B'\n","    elif row['Class'] == 'Class C':\n","        return True, row['Class']\n","    else:\n","        return False, row['Class']\n","\n","# Apply the function to create the 'Multiple Witnesses' and 'Class' columns\n","df['Multiple Witnesses'], df['Class'] = zip(*df.apply(set_multiple_witnesses, axis=1))\n","\n","# Save the updated dataset\n","df.to_csv('../Data/task_4_dataset', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"2qB4DMlpolL8"},"source":["# Task 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t3rCSDR1olL8"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the BFRO dataset\n","bfro_df = pd.read_csv('../Data/task_4_dataset.tsv', delimiter='\\t')\n","\n","# Load the environment dataset\n","environment_df = pd.read_csv('../Data/environment.tsv', delimiter='\\t')\n","\n","# Ensure the 'Year' column in BFRO is of the same data type for merging\n","bfro_df['Year'] = bfro_df['Year'].astype(str)\n","\n","# For 'Year'-independent features, create a DataFrame indexed by 'State'\n","state_based_features = environment_df[['State', 'Forest cover rate', 'Number of lakes']].drop_duplicates(subset=['State'])\n","\n","# Join the BFRO dataset with state-based features on 'State' to fill in these values\n","bfro_df = pd.merge(bfro_df, state_based_features, on='State', how='left')\n","\n","# For the 'Year'-dependent feature ('Total Disasters'), we need to ensure it's correctly aligned by 'Year'\n","# Extract the 'Total Disasters' feature with 'Year' and 'State'\n","year_based_features = environment_df[['Year', 'State', 'Total Disasters']].copy()  # Use .copy() to explicitly create a copy\n","\n","# Convert 'Year' in this DataFrame to string to match the BFRO 'Year' column\n","year_based_features['Year'] = year_based_features['Year'].astype(str)\n","\n","# Merge this with the already enhanced BFRO DataFrame\n","bfro_df = pd.merge(bfro_df, year_based_features, on=['Year', 'State'], how='left')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iie1AB-iolL8"},"outputs":[],"source":["import requests\n","\n","# Fetch and load the species distribution JSON data.\n","url = \"https://data.ny.gov/api/views/tk82-7km5/rows.json?accessType=DOWNLOAD\"\n","response = requests.get(url)\n","species_data = response.json()\n","\n","# Create a DataFrame from the species distribution data.\n","species_df = pd.DataFrame.from_records(species_data['data'])\n","\n","# Define the column headers as they appear in the JSON structure.\n","# This will depend on the structure of your JSON data.\n","column_headers = species_data['meta']['view']['columns']\n","columns = [col['name'] for col in column_headers]\n","species_df.columns = columns\n","\n","# Define a function to safely parse the year and handle 'not available'\n","def parse_year(year_str):\n","    try:\n","        # If there's a range, take the latest year\n","        if isinstance(year_str, str) and '-' in year_str:\n","            return int(year_str.split('-')[1])\n","        # If it's a single year, return it as an integer\n","        return int(year_str)\n","    except ValueError:\n","        # If it's 'not available' or any other non-numeric value, return None or a default value\n","        return None\n","\n","# Apply the function to the 'Year Last Documented' column\n","species_df['Year Last Documented Max'] = species_df['Year Last Documented'].apply(parse_year)\n","\n","# Assuming species_df has been created and parsed correctly\n","\n","# Group by 'County' to calculate the 'Wildlife Diversity'\n","wildlife_diversity = species_df.groupby('County').agg(Wildlife_Diversity=('Scientific Name', 'nunique')).reset_index()\n","\n","# Define a function to check for significant conservation concern\n","def check_concern(row):\n","    concern_statuses = ['Endangered', 'Threatened']  # Define statuses that denote concern\n","    # Check if either NY or Federal listing status is in our concern_statuses\n","    if row['NY Listing Status'] in concern_statuses or row['Federal Listing Status'] in concern_statuses:\n","        return 1  # Significant conservation concern\n","    else:\n","        return 0  # No significant concern\n","\n","# Apply the function to each row of the DataFrame\n","species_df['Conservation Concern'] = species_df.apply(check_concern, axis=1)\n","\n","# Group by 'County' and find if there's any conservation concern within each county\n","conservation_concern = species_df.groupby('County')['Conservation Concern'].max().reset_index()\n","\n","# Group by 'County' to find the most recent 'Year Last Documented'\n","last_documented_presence = species_df.groupby('County').agg(Last_Documented_Presence=('Year Last Documented Max', 'max')).reset_index()\n","\n","# Merge the new features into the BFRO dataset based on the 'County' column.\n","bfro_df = bfro_df.merge(wildlife_diversity, on='County', how='left')\n","bfro_df = bfro_df.merge(conservation_concern, on='County', how='left')\n","bfro_df = bfro_df.merge(last_documented_presence, on='County', how='left')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HkrROq6olL8","outputId":"4539c096-f094-448f-a5f5-5d43cfe81586"},"outputs":[{"name":"stdout","output_type":"stream","text":["hello\n","        Report Type     Id    Class                Submitted Date  \\\n","0            Report  13038  Class A   Saturday, November 12, 2005   \n","1            Report   8792  Class B        Thursday, June 3, 2004   \n","2            Report   1255  Class B      Tuesday, October 5, 1999   \n","3            Report  11616  Class B           Friday, May 6, 2005   \n","4            Report    637  Class A     Monday, November 13, 2000   \n","...             ...    ...      ...                           ...   \n","5495  Media Article    519      NaN  Wednesday, November 15, 2006   \n","5496  Media Article    659      NaN      Monday, January 14, 2013   \n","5497  Media Article    733      NaN        Tuesday, March 4, 1919   \n","5498  Media Article    678      NaN    Wednesday, January 1, 2014   \n","5499  Media Article     59      NaN      Wednesday, June 18, 1980   \n","\n","                                               Headline  Year  Season  \\\n","0     Snowmobiler has encounter in deep snow near Po...  2005  Winter   \n","1     Four nocturnal hikers get pelted with snow nea...  2004  Winter   \n","2     Creature observed walking back and forth by wi...  1999    Fall   \n","3              Fishermen find footprints east of Egegik  2005  Summer   \n","4     Campers' encounter just after dark in the Wran...  2000  Summer   \n","...                                                 ...   ...     ...   \n","5495                Wisconsin Boys Say They Saw Bigfoot   nan     NaN   \n","5496  Back to theNorth Wood column: Strange creature...   nan     NaN   \n","5497              Return of Wildman Revives Old Terrors   nan     NaN   \n","5498                           Nicholas County Bigfoot?   nan     NaN   \n","5499          Two Men Report Seeing Huge Hairy Creature   nan     NaN   \n","\n","         Month   State            County  ... ZipCode Forest cover rate  \\\n","0     November  Alaska         Anchorage  ...     NaN             30.4%   \n","1         June  Alaska         Anchorage  ...     NaN             30.4%   \n","2      October  Alaska            Bethel  ...     NaN             30.4%   \n","3          May  Alaska       Bristol Bay  ...     NaN             30.4%   \n","4     November  Alaska  Cordova-McCarthy  ...     NaN             30.4%   \n","...        ...     ...               ...  ...     ...               ...   \n","5495       NaN     NaN               NaN  ...     NaN               NaN   \n","5496       NaN     NaN               NaN  ...     NaN               NaN   \n","5497       NaN     NaN               NaN  ...     NaN               NaN   \n","5498       NaN     NaN               NaN  ...     NaN               NaN   \n","5499       NaN     NaN               NaN  ...     NaN               NaN   \n","\n","     Number of lakes Total Disasters Wildlife_Diversity Conservation Concern  \\\n","0          over 2000             0.0                NaN                  NaN   \n","1          over 2000             0.0                NaN                  NaN   \n","2          over 2000             0.0                NaN                  NaN   \n","3          over 2000             0.0                NaN                  NaN   \n","4          over 2000             0.0                NaN                  NaN   \n","...              ...             ...                ...                  ...   \n","5495             NaN             NaN                NaN                  NaN   \n","5496             NaN             NaN                NaN                  NaN   \n","5497             NaN             NaN                NaN                  NaN   \n","5498             NaN             NaN                NaN                  NaN   \n","5499             NaN             NaN                NaN                  NaN   \n","\n","     Last_Documented_Presence City/Suburb/Town/Rural Unemployment_rate  \\\n","0                         NaN                   City               5.5   \n","1                         NaN                   City               5.9   \n","2                         NaN                    NaN               NaN   \n","3                         NaN                  Rural               5.9   \n","4                         NaN                    NaN               NaN   \n","...                       ...                    ...               ...   \n","5495                      NaN                    NaN               NaN   \n","5496                      NaN                    NaN               NaN   \n","5497                      NaN                    NaN               NaN   \n","5498                      NaN                    NaN               NaN   \n","5499                      NaN                    NaN               NaN   \n","\n","     Civilian labor force  \n","0                 148,840  \n","1                 146,603  \n","2                     NaN  \n","3                     969  \n","4                     NaN  \n","...                   ...  \n","5495                  NaN  \n","5496                  NaN  \n","5497                  NaN  \n","5498                  NaN  \n","5499                  NaN  \n","\n","[5500 rows x 53 columns]\n"]}],"source":["# Load the unemployment dataset\n","df_unemployment = pd.read_csv('../Data/unemployment.csv', delimiter=',')\n","\n","# print(df_unemployment.columns)\n","\n","# Filter columns to keep\n","columns_keep = ['Area_name', 'State', 'City/Suburb/Town/Rural',\n","                'Unemployment_rate_2000', 'Unemployment_rate_2001', 'Unemployment_rate_2002', 'Unemployment_rate_2003',\n","                'Unemployment_rate_2004', 'Unemployment_rate_2005', 'Unemployment_rate_2006', 'Unemployment_rate_2007',\n","                'Unemployment_rate_2008', 'Unemployment_rate_2009', 'Unemployment_rate_2010', 'Unemployment_rate_2011',\n","                'Unemployment_rate_2012', 'Unemployment_rate_2013', 'Unemployment_rate_2014', 'Unemployment_rate_2015',\n","                'Unemployment_rate_2016', 'Unemployment_rate_2017', 'Unemployment_rate_2018', 'Unemployment_rate_2019',\n","                'Unemployment_rate_2020', 'Median_Household_Income_2019']\n","\n","# Filter columns to keep\n","columns_civilian = ['Area_name', 'State', 'City/Suburb/Town/Rural',\n","                    'Civilian_labor_force_2000', 'Civilian_labor_force_2001', 'Civilian_labor_force_2002',\n","                    'Civilian_labor_force_2003', 'Civilian_labor_force_2004', 'Civilian_labor_force_2005',\n","                    'Civilian_labor_force_2006', 'Civilian_labor_force_2007', 'Civilian_labor_force_2008',\n","                    'Civilian_labor_force_2009', 'Civilian_labor_force_2010', 'Civilian_labor_force_2011',\n","                    'Civilian_labor_force_2012', 'Civilian_labor_force_2013', 'Civilian_labor_force_2014',\n","                    'Civilian_labor_force_2015', 'Civilian_labor_force_2016', 'Civilian_labor_force_2017',\n","                    'Civilian_labor_force_2018', 'Civilian_labor_force_2019', 'Civilian_labor_force_2020',\n","                    'Median_Household_Income_2019']\n","\n","# Make a new df with the columns to keep\n","df_filtered = df_unemployment[columns_keep].copy()\n","df_civilian = df_unemployment[columns_civilian].copy()\n","\n","# Extract year from column names and create a new 'Year' column\n","years = df_filtered.columns.str.extract(r'(\\d{4})').astype(float)  # Extract years as float\n","valid_years = years.dropna().astype(int)  # Drop NaN and convert to int\n","df_filtered['Year'] = valid_years  # Assign valid years to 'Year' column\n","\n","# Extract year from column names and create a new 'Year' column\n","years = df_civilian.columns.str.extract(r'(\\d{4})').astype(float)  # Extract years as float\n","valid_years = years.dropna().astype(int)  # Drop NaN and convert to int\n","df_civilian['Year'] = valid_years  # Assign valid years to 'Year' column\n","\n","# Melt the dataframe to have one row per year per area\n","df_melted = df_filtered.melt(id_vars=['Area_name', 'State', 'City/Suburb/Town/Rural', 'Median_Household_Income_2019',\n","                                      'Year'],\n","                             value_vars=[col for col in df_filtered.columns if col.startswith('Unemployment_rate')],\n","                             var_name='Variable', value_name='Unemployment_rate')\n","\n","df_civilian_melted = df_civilian.melt(id_vars=['Area_name', 'State', 'City/Suburb/Town/Rural',\n","                                               'Median_Household_Income_2019', 'Year'],\n","                                      value_vars=[col for col in df_civilian.columns if\n","                                                  col.startswith('Civilian_labor_force')],\n","                                      var_name='Variable', value_name='Civilian labor force')\n","\n","# Extract year from 'Variable' column and assign it to 'Year' column\n","df_melted['Year'] = df_melted['Variable'].str.extract(r'(\\d{4})').astype(int)\n","\n","df_civilian_melted['Year'] = df_civilian_melted['Variable'].str.extract(r'(\\d{4})').astype(int)\n","\n","# Sort the dataframe by 'Area_name', 'State', and 'Year'\n","df_melted.sort_values(by=['Area_name', 'State', 'Year'], inplace=True)\n","\n","df_civilian_melted.sort_values(by=['Area_name', 'State', 'Year'], inplace=True)\n","\n","# Filter the DataFrame to drop rows where 'Area_name' contains only state names\n","df_melted = df_melted[~(df_melted['Area_name'].str.match(r'^[A-Za-z\\s]+$') & ~df_melted['Area_name'].str.contains(','))]\n","\n","df_civilian_melted = df_civilian_melted[~(df_civilian_melted['Area_name'].str.match(r'^[A-Za-z\\s]+$') &\n","                                          ~df_civilian_melted['Area_name'].str.contains(','))]\n","\n","# Split by comma and keep only the first part\n","df_melted['Area_name'] = df_melted['Area_name'].str.split(',').str[0].str.strip().str.strip('\"')\n","df_melted['Area_name'] = df_melted['Area_name'].str.replace(r',\\s*[A-Z]{2}', '')\n","\n","df_civilian_melted['Area_name'] = df_civilian_melted['Area_name'].str.split(',').str[0].str.strip().str.strip('\"')\n","df_civilian_melted['Area_name'] = df_civilian_melted['Area_name'].str.replace(r',\\s*[A-Z]{2}', '')\n","\n","# Create a function to remove the last word from a string\n","def remove_last_word(text):\n","    # Split the string by whitespace\n","    words = text.split()\n","    # Remove the last word\n","    modified_text = ' '.join(words[:-1])\n","    return modified_text\n","\n","# Apply the function to the 'Area_name' column\n","df_melted['Area_name'] = df_melted['Area_name'].apply(remove_last_word)\n","\n","df_civilian_melted['Area_name'] = df_civilian_melted['Area_name'].apply(remove_last_word)\n","\n","# Display unique values in the 'State' column before mapping\n","# print(\"Unique state abbreviations before mapping:\")\n","# print(df_melted['State'].unique())\n","\n","# Map state abbreviations to full names\n","abbr_to_full = {\n","    'AL': 'Alabama',\n","    'AK': 'Alaska',\n","    'AZ': 'Arizona',\n","    'AR': 'Arkansas',\n","    'CA': 'California',\n","    'CO': 'Colorado',\n","    'CT': 'Connecticut',\n","    'DE': 'Delaware',\n","    'FL': 'Florida',\n","    'GA': 'Georgia',\n","    'HI': 'Hawaii',\n","    'ID': 'Idaho',\n","    'IL': 'Illinois',\n","    'IN': 'Indiana',\n","    'IA': 'Iowa',\n","    'KS': 'Kansas',\n","    'KY': 'Kentucky',\n","    'LA': 'Louisiana',\n","    'ME': 'Maine',\n","    'MD': 'Maryland',\n","    'MA': 'Massachusetts',\n","    'MI': 'Michigan',\n","    'MN': 'Minnesota',\n","    'MS': 'Mississippi',\n","    'MO': 'Missouri',\n","    'MT': 'Montana',\n","    'NE': 'Nebraska',\n","    'NV': 'Nevada',\n","    'NH': 'New Hampshire',\n","    'NJ': 'New Jersey',\n","    'NM': 'New Mexico',\n","    'NY': 'New York',\n","    'NC': 'North Carolina',\n","    'ND': 'North Dakota',\n","    'OH': 'Ohio',\n","    'OK': 'Oklahoma',\n","    'OR': 'Oregon',\n","    'PA': 'Pennsylvania',\n","    'RI': 'Rhode Island',\n","    'SC': 'South Carolina',\n","    'SD': 'South Dakota',\n","    'TN': 'Tennessee',\n","    'TX': 'Texas',\n","    'UT': 'Utah',\n","    'VT': 'Vermont',\n","    'VA': 'Virginia',\n","    'WA': 'Washington',\n","    'WV': 'West Virginia',\n","    'WI': 'Wisconsin',\n","    'WY': 'Wyoming',\n","    'PR': 'Puerto Rico'\n","}\n","\n","# Replace state abbreviations with full names using the mapping dictionary\n","df_melted['State'] = df_melted['State'].map(abbr_to_full)\n","\n","df_civilian_melted['State'] = df_civilian_melted['State'].map(abbr_to_full)\n","\n","# Display unique values in the 'State' column after mapping\n","# print(\"\\nUnique state names after mapping:\")\n","# print(df_melted['State'].unique())\n","\n","# print(df_melted['Area_name'])\n","print(\"hello\")\n","\n","# Drop the 'Variable' column\n","df_melted.drop(columns=['Variable'], inplace=True)\n","\n","df_civilian_melted.drop(columns=['Variable'], inplace=True)\n","\n","# Rename the 'Area_name' column to 'County'\n","df_melted.rename(columns={'Area_name': 'County'}, inplace=True)\n","\n","df_civilian_melted.rename(columns={'Area_name': 'County'}, inplace=True)\n","\n","# Merge the two melted dataframes on 'County', 'State', 'City/Suburb/Town/Rural', and 'Year'\n","df_melted2 = df_melted.merge(df_civilian_melted,\n","                             on=['County', 'State', 'City/Suburb/Town/Rural', 'Year'],\n","                             how='outer')\n","\n","# Drop the 'Median_Household_Income_2019_x' and 'Median_Household_Income_2019_y' columns\n","df_melted2.drop(columns=['Median_Household_Income_2019_x', 'Median_Household_Income_2019_y'], inplace=True)\n","\n","# Save the merged DataFrame to a CSV file\n","# df_melted2.to_csv('merged_unemployment_civilian.csv', index=False)\n","\n","# Replace \"Southeast Fairbanks Census\" with \"Southeast Fairbanks\" in the 'County' column\n","df_melted2['County'] = df_melted2['County'].replace('Southeast Fairbanks Census', 'Southeast Fairbanks')\n","\n","df_melted2['County'] = df_melted2['County'].replace('Prince of Wales-Hyder Census', 'Prince of Wales')\n","\n","df_melted2['County'] = df_melted2['County'].replace('Fairbanks North Star', 'Fairbanks')\n","\n","df_melted2['County'] = df_melted2['County'].replace('Etowah', 'Etowa')\n","\n","# Save the DataFrame to a CSV file\n","df_melted2.to_csv('merged_unemployment_civilian.csv', index=False)\n","\n","# Convert 'Year' column in df_melted2 to string to match the 'Year' column in bfro_df\n","df_melted2['Year'] = df_melted2['Year'].astype(str)\n","\n","# Perform left merge on 'County', 'Year', and 'State'\n","df_merged = bfro_df.merge(df_melted2, on=['County', 'Year', 'State'], how='left')\n","\n","# Check the merged DataFrame\n","print(df_merged)\n","\n","# Save df to a csv file\n","df_merged.to_csv('merged_bfro.csv', index=False)\n","# convert to a tsv file\n","df_merged.to_csv('merged_bfro.tsv', sep='\\t', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6WPOiHUolL9"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}